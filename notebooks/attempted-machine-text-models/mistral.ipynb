#  Step 1: Install Required Libraries
!pip install datasets transformers langid pandas tqdm accelerate bitsandbytes --quiet

#  Step 2: Clone and Load Aligned Vuk'uzenzele Data
import os
import pandas as pd

!git clone https://github.com/dsfsi/vukuzenzele-nlp.git || echo "Repo already cloned."

data_dir = "vukuzenzele-nlp/data/sentence_align_output"
desired_langs = ['ven', 'xho', 'nso', 'tso']
data = []

csv_files = [f for f in os.listdir(data_dir) if f.endswith(".csv")]
for file in csv_files:
    langs = file.split("_")[1].split(".")[0].split("_")
    if any(lang in desired_langs for lang in langs):
        try:
            df = pd.read_csv(os.path.join(data_dir, file))
            df["src_lang"] = langs[0]
            df["tgt_lang"] = langs[1] if len(langs) > 1 else langs[0]
            df = df[["src_text", "tgt_text", "cosine_score", "src_lang", "tgt_lang"]]
            data.append(df)
        except Exception as e:
            print(f" Error in {file}: {e}")

df = pd.concat(data, ignore_index=True)

#  Step 3: Filter Human Texts
languages = ['ven', 'xho', 'nso', 'tso']
filtered = df[df['src_lang'].isin(languages)].copy()
filtered['language'] = filtered['src_lang']
filtered['type'] = 'human_text'

#  Step 4: Load Mistral-7B Instruct Model
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_name = "mistralai/Mistral-7B-Instruct-v0.2"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype="auto")

generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

#  Step 5: Prompts per Language
PROMPTS = {
    "ven": ["Sumarisani mafhungo a muvhuso."],
    "xho": ["Chaza iphulo likarhulumente lezempilo."],
    "nso": ["Ngwala ka ga lenaneo la mmuÅ¡o la thuto."],
    "tso": ["Nyika nhlamuselo ya pulani ya vurhangeri bya rixaka."]
}

#  Step 6: Generate Machine Text
from tqdm.notebook import tqdm
import random
import langid

results = []
max_per_lang = 200

for lang in languages:
    print(f"\nðŸŒ€ Generating for '{lang}'")
    count = 0
    seen_texts = set()
    pbar = tqdm(total=max_per_lang, desc=f"{lang.upper()} Progress")
    
    while count < max_per_lang:
        prompt = random.choice(PROMPTS[lang])
        full_prompt = f"Generate a short government text in {lang}.\nPrompt: {prompt}\nResponse:"
        try:
            output = generator(
                full_prompt,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.9,
                top_k=40,
                top_p=0.92
            )
            raw_text = output[0]['generated_text'].replace(full_prompt, '').strip()

            if (
                raw_text and
                12 <= len(raw_text.split()) <= 40 and
                langid.classify(raw_text)[0] == lang and
                raw_text not in seen_texts
            ):
                results.append({
                    "src_text": raw_text,
                    "label": 1,
                    "language": lang
                })
                seen_texts.add(raw_text)
                count += 1
                pbar.update(1)
        except Exception as e:
            print(f"âš ï¸ Error: {e}")
            continue

    pbar.close()

#  Step 7: Save Combined Dataset
machine_df = pd.DataFrame(results)
human_df = filtered[['src_text', 'language']]
human_df['label'] = 0

final_df = pd.concat([human_df, machine_df], ignore_index=True)
final_df = final_df.sample(frac=1, random_state=42)

final_df.to_csv("vukuzenzele_labeled_mistral.csv", index=False)
print(" Final dataset saved to 'vukuzenzele_labeled_mistral.csv'")
