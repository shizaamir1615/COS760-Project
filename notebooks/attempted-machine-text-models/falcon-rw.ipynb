#  Step 1: Install Required Libraries
!pip install datasets transformers langid pandas tqdm --quiet

#  Step 2: Clone and Load Aligned Vuk'uzenzele Data
import os
import pandas as pd

!git clone https://github.com/dsfsi/vukuzenzele-nlp.git || echo "Repo already cloned."

data_dir = "vukuzenzele-nlp/data/sentence_align_output"
desired_langs = ['ven', 'xho', 'nso', 'tso']
data = []

csv_files = [f for f in os.listdir(data_dir) if f.endswith(".csv")]
for file in csv_files:
    langs = file.split("_")[1].split(".")[0].split("_")
    if any(lang in desired_langs for lang in langs):
        try:
            df = pd.read_csv(os.path.join(data_dir, file))
            df["src_lang"] = langs[0]
            df["tgt_lang"] = langs[1] if len(langs) > 1 else langs[0]
            df = df[["src_text", "tgt_text", "cosine_score", "src_lang", "tgt_lang"]]
            data.append(df)
        except Exception as e:
            print(f" Error in {file}: {e}")

df = pd.concat(data, ignore_index=True)

#  Step 3: Filter Human Texts for Selected Languages
languages = ['ven', 'xho', 'nso', 'tso']
filtered = df[df['src_lang'].isin(languages)].copy()
filtered['language'] = filtered['src_lang']
filtered['type'] = 'human_text'

#  Step 4: Load Multilingual Generator (Falcon RW or try BLOOM)
from transformers import pipeline
generator = pipeline("text-generation", model="tiiuae/falcon-rw-1b")

#  Step 5: Prompts per Language
PROMPTS = {
    "ven": [
        "Sumarisani mafhungo a muvhuso.",
        "Bvela phanda nga ha mushumo wa muvhuso.",
        "Ambani nga ha thendelo ya zwa mutakalo.",
        "Bvuma mafhungo a uri hani muvhuso u thusa vhathu.",
        "Fhinduwa nga ha ndangulo ya zwa ndeme."
    ],
    "xho": [
        "Chaza iphulo likarhulumente lezempilo.",
        "Ngaba ungachaza uhlelo lukarhulumente kwezempilo?",
        "Yenza isishwankathelo sesicwangciso sezempilo sikarhulumente.",
        "Bhala inqaku malunga neenkonzo zezempilo zikarhulumente.",
        "Yenza ingxelo emfutshane ngesicwangciso sezempilo."
    ],
    "nso": [
        "Ngwala ka ga lenaneo la mmušo la thuto.",
        "Efa kakaretso ka lenaneo la mmušo la tlhokomelo ya bophelo.",
        "Ngwala ka ga dikgwebo tše mmušo o di thekgo.",
        "Hlalosa maikemišo a mmušo mabapi le thuto.",
        "Ngwala ka ga lenaneo la paballo ya bophelo."
    ],
    "tso": [
        "Nyika nhlamuselo ya pulani ya vurhangeri bya rixaka.",
        "Hlamusela ndlela leyi mfumo wu pfunaka vaaki ha yona.",
        "Suma xitiviso xa swa rihanyo lexi humaka eka mfumo.",
        "Kambisisa pulani ya ntirho wa mfumo.",
        "Tlhantlha nhlamuselo ya xikongomelo xa mfumo."
    ]
}

#  Step 6: Generate Machine Text
from tqdm.notebook import tqdm
from datetime import datetime
import random
import langid

results = []
max_per_lang = 200  # adjust as needed

for lang in languages:
    print(f"\n🌀 Generating for '{lang}'")
    count = 0
    seen_texts = set()
    pbar = tqdm(total=max_per_lang, desc=f"{lang.upper()} Progress")
    
    while count < max_per_lang:
        prompt = random.choice(PROMPTS[lang])
        try:
            output = generator(
                prompt,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.9,
                top_k=40,
                top_p=0.92,
                pad_token_id=generator.tokenizer.eos_token_id
            )
            raw_text = output[0]['generated_text'].strip()
            gen_text = raw_text.replace(prompt, '').strip()

            print(f"\n [{lang}] Generated: {gen_text[:60]}...")

            if (
                gen_text and
                12 <= len(gen_text.split()) <= 40 and
                langid.classify(gen_text)[0] == lang and
                gen_text not in seen_texts
            ):
                results.append({
                    "src_text": gen_text,
                    "label": 1,
                    "language": lang
                })
                seen_texts.add(gen_text)
                count += 1
                pbar.update(1)
            else:
                print(f" Skipped | Words: {len(gen_text.split())} | Lang: {langid.classify(gen_text)[0]}")
        except Exception as e:
            print(f" Error: {e}")
            continue

    pbar.close()

#  Step 7: Save the Combined Dataset
machine_df = pd.DataFrame(results)
human_df = filtered[['src_text', 'language']]
human_df['label'] = 0

final_df = pd.concat([human_df, machine_df], ignore_index=True)
final_df = final_df.sample(frac=1, random_state=42)

final_df.to_csv("vukuzenzele_labeled_sampled.csv", index=False)
print(" Final dataset saved to 'vukuzenzele_labeled_sampled.csv'")
